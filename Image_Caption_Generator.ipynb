{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahEldreny/Image-Captioning-Project-/blob/main/Image_Caption_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNu58xJCcpGY"
      },
      "source": [
        "# Image Caption Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeCs8XaYcpGl"
      },
      "source": [
        "## Whenever an image appears in front of us, our brain can annotate or label it. But what about computers? How can a machine process and label an image with a highly relevant and accurate caption? It seemed quite impossible a few years back. Still, with the enhancement of [Computer Vision](https://www.analyticsvidhya.com/blog/2021/06/everything-happening-in-computer-vision-that-you-should-know/) and [Deep learning](https://www.analyticsvidhya.com/blog/2021/12/a-guide-on-deep-learning-from-basics-to-advanced-concepts/) algorithms, the availability of relevant datasets, and AI models, it becomes easier to build a relevant caption generator for an image. Even Caption generation is growing worldwide, and many data annotation firms are earning billions. In this guide, we will build one such annotation tool capable of generating relevant captions for the image with the help of datasets. Basic knowledge of two Deep learning techniques, including [LSTM](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/) and [CNN](https://www.analyticsvidhya.com/blog/2021/07/convolution-neural-network-better-understanding/), is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK-GL0SJcpGn"
      },
      "source": [
        "## Table of contents :\n",
        "   \n",
        "● What is Image Caption Generator?\n",
        "\n",
        "● What is CNN?\n",
        "\n",
        "● What is LSTM?\n",
        "\n",
        "● Dataset for Image Caption Generator :\n",
        "\n",
        "      1- Pre-requisites\n",
        "      2- Building the Image Caption Generator\n",
        "\n",
        "● Import all the Required Packages\n",
        "\n",
        "● Perform Data Cleaning\n",
        "\n",
        "● Extract the Feature Vector\n",
        "\n",
        "● Loading dataset for model training\n",
        "\n",
        "● Tokenizing the Vocabulary\n",
        "\n",
        "● Create a Data generator\n",
        "\n",
        "● Define the CNN-RNN model\n",
        "\n",
        "● Training the Image Caption Generator model\n",
        "\n",
        "● Testing the Image Caption Generator model\n",
        "\n",
        "● End Note\n",
        "\n",
        "● Frequently Asked Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El9MqimycpGs"
      },
      "source": [
        "## What is Image Caption Generator?\n",
        "Image caption generator is a process of recognizing the context of an image and annotating it with relevant captions using deep learning and computer vision. It includes labeling an image with English keywords with the help of datasets provided during model training. The imagenet dataset trains the CNN model called Xception. Xception is responsible for image feature extraction. These extracted features will be fed to the LSTM model, which generates the image caption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwh2VKALcpGv"
      },
      "source": [
        "![image.png](https://i.stack.imgur.com/XygNZ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmQPfYINcpGw"
      },
      "source": [
        "## What is CNN?\n",
        "CNN is a subfield of Deep learning and specialized deep neural networks used to recognize and classify images. It processes the data represented as 2D matrix-like images. CNN can deal with scaled, translated, and rotated imagery. It analyzes the visual imagery by scanning them from left to right and top to bottom and extracting relevant features. Finally, it combines all the parts for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu0wsIcrcpGy"
      },
      "source": [
        "## What is LSTM?\n",
        "Being a type of RNN (recurrent neural network), LSTM (Long short-term memory) is capable of working with sequence prediction problems. It is mostly used for the next word prediction purposes, as in Google search our system is showing the next word based on the previous text. Throughout the processing of inputs, LSTM is used to carry out the relevant information and to discard non-relevant information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL-1h76lcpGz"
      },
      "source": [
        "● CNN – To extract features from the image. A pre-trained model called Xception is used for this.\n",
        "\n",
        "● LSTM – To generate a description from the extracted information of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj_HROTycpG2"
      },
      "source": [
        "## Dataset for Image Caption Generator\n",
        "The Flickr_8K dataset represents the model training of image caption generators. The dataset is downloaded directly from the below links. The downloading process takes some time due to the dataset’s large size(1GB). In the image below, you can check all the files in the Flickr_8k_text folder. The most important file is Flickr 8k.token, which stores all the image names with captions. 8091 images are stored inside the Flicker8k_Dataset folder and the text files with captions of images are stored in the Flickr_8k_text folder.\n",
        "\n",
        "● [Flicker8k_Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k/download?datasetVersionNumber=1)\n",
        "\n",
        "\n",
        "# Pre-requisites\n",
        "We will use Jupyter notebooks to run our caption generator, A good understanding of Python, Deep learning, and NLP is required for the implementation. If you’re not familiar with these techniques. Please refer to the pre_requisites below first.\n",
        "\n",
        "● [Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/)\n",
        "\n",
        "● [DeepLearning](https://www.analyticsvidhya.com/blog/2021/05/a-comprehensive-tutorial-on-deep-learning-part-1/)\n",
        "\n",
        "● [NLP](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)\n",
        "\n",
        "# Install below libraries, to begin with, the project:\n",
        "\n",
        "pip install TensorFlow\n",
        "\n",
        "pip install Keras\n",
        "\n",
        "pip install pillow\n",
        "\n",
        "pip install NumPy\n",
        "\n",
        "pip install tqdm\n",
        "\n",
        "pip install jupyterlab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnJ2ULG0cpG3"
      },
      "source": [
        "# Import Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04C0-x9zG-40",
        "outputId": "5d71753f-a342-4f01-a296-bce6fb0ccd15"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL970BBVn7Ot",
        "outputId": "a828c062-6c88-4fa8-d26d-c5a4b39ff241"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Flicker8k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dntNAZsnliMu",
        "outputId": "6a71071c-55c6-4945-9c8e-6b1ece310be6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Flicker8k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flicker8k_Dataset.zip\n",
        "[n]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErGX-VGhIivE",
        "outputId": "b04aea83-1b18-49d5-cd34-7f483a7006cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Flicker8k_Dataset.zip\n",
            "replace Images/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExfbnPo_cpG5"
      },
      "outputs": [],
      "source": [
        "import numpy as np      # Provides support for efficient numerical operations.\n",
        "from PIL import Image   # Handle image processing tasks.\n",
        "import os               # Allows interaction with the operating system by providing functions for file/folder manipulation.\n",
        "import pickle           # Store NumPy features\n",
        "from pickle import dump, load   # Enables saving/loading data structures like models or tokenizers.\n",
        "from keras.applications.xception import Xception  # Which is a pre-trained CNN, along with preprocessing functions specific to Xception.\n",
        "from keras.applications.xception import preprocess_input\n",
        "from keras.preprocessing import image  # For loading imgs and img processing\n",
        "from keras.utils import load_img\n",
        "from keras.utils import img_to_array   # Applying image transformations\n",
        "from keras.preprocessing.text import Tokenizer  # For text preprocessing/tokenization\n",
        "from keras.utils import pad_sequences    # Importing utility functions related to sequence padding, one-hot encoding\n",
        "from keras.utils import to_categorical, plot_model   #          of labels, and embedding layer in Keras.\n",
        "from keras.layers import add          # Function for merging layers.\n",
        "from keras.models import Model, load_model      # Creating/loading models.\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout   # Building the neural network model.\n",
        "from tqdm import tqdm          # Provides a progress bar visualization during iterations.\n",
        "from nltk.translate.bleu_score import corpus_bleu     # calculate the BLEU score for a corpus of multiple sentences rather than individual sentences\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFIjO7R4cpG-"
      },
      "source": [
        "➔ NumPy : Provides support for efficient numerical operations\n",
        "\n",
        "➔ PIL : Handle image processing tasks\n",
        "\n",
        "➔ os : Allows interaction with the operating system by providing functions for file/folder manipulation\n",
        "\n",
        "➔ pickle : used to store numpy features extracted\n",
        "\n",
        "➔ keras :  provide a user-friendly and intuitive interface for building, training, evaluating, and deploying deep learning models\n",
        "\n",
        "➔ Tokenizer : used for loading the text as convert them into a token\n",
        "\n",
        "➔ tqdm : Progress bar decorator for iterators\n",
        "\n",
        "➔ Xception : For feature extraction from the image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb8BINtzcpG_"
      },
      "source": [
        "# Loading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8TtLjGacpHA"
      },
      "outputs": [],
      "source": [
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Flicker8k'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLTWetZmcpHD"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuKaikDwcpHF"
      },
      "source": [
        "Loading and Restructure the model\n",
        "\n",
        "Xception is a deep convolutional neural network (CNN) architecture that was introduced by François Chollet in 2016. It stands for \"Extreme Inception\" and is inspired by the Inception architecture.\n",
        "The Xception model aims to improve upon the traditional Inception module by replacing it with an extreme version called the \"depthwise separable convolution\". This approach separates the learning of spatial correlations and channel-wise relationships, allowing for more efficient computation and better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo4l8xnTcpHH"
      },
      "outputs": [],
      "source": [
        "# Load the Model\n",
        "model = Xception()\n",
        "\n",
        "# Restructure model\n",
        "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
        "\n",
        "# Summerize\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vltxDscNcpHJ"
      },
      "source": [
        "## Extract the image features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ6HCrRlcpHK"
      },
      "source": [
        "Here we extract features from images using a pre-trained model, presumably Xception, and load the data for preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-veizdQcpHL"
      },
      "outputs": [],
      "source": [
        "# extract features from image\n",
        "features = {}\n",
        "directory = os.path.join('/content/drive/My Drive/Flicker8k/Images')\n",
        "\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "    # load the image from file\n",
        "    img_path = directory + '/' + img_name\n",
        "    image = load_img(img_path, target_size=(299, 299))\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # preprocess image for Xception\n",
        "    # subtracting mean pixel values specific to Xception network\n",
        "    image = preprocess_input(image)\n",
        "    # extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    # get image ID\n",
        "    image_id = img_name.split('.')[0]\n",
        "    # store feature\n",
        "    features[image_id] = feature\n",
        "\n",
        "    '''After running this cell, you will have a dictionary (features)\n",
        "        where each key represents an image ID, and each value represents\n",
        "        its corresponding extracted feature vector obtained from the Xception model'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QufLcrh1cpHM"
      },
      "source": [
        "- features is an empty dictionary that will store the extracted features for each image\n",
        "\n",
        "- directory specifies the path to the directory containing your images\n",
        "\n",
        "- The loop iterates over each file name (img_name) in the specified directory\n",
        "\n",
        "- loading an individual image file (img_path) using load_img() function from Keras. It also resizes the loaded image to a target size of (299, 299) pixels\n",
        "\n",
        "- image is converted into a NumPy array using img_to_array() function\n",
        "\n",
        "- image is reshaped to match the expected input shape of the model\n",
        "\n",
        "- The ID of each image (extracted from its file name) is stored in image_id, (img_name.split('.')[0]) split of the image name from the extension to load only the image name\n",
        "\n",
        "- Finally, the extracted feature vector for each image is stored in the dictionary features with its corresponding ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCq6IQAEcpHM"
      },
      "source": [
        "\n",
        "Re-extraction of features can extend running time, Dumps and store your dictionary in a pickle for reloading it to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSyulgPMcpHN"
      },
      "outputs": [],
      "source": [
        "# store features in pickle\n",
        "pickle.dump(features, open(os.path.join('/content/drive/My Drive/Flicker8k','features.pkl'), 'wb'))    #  saves the Python object called \"features\" into a binary file at the specified location\n",
        "\n",
        "'''After executing these lines of code, your \"features\" object will be saved as a pickled binary file named \"features.pkl\" at the specified location (file_path).'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhARas6EcpHO"
      },
      "source": [
        "- pickle.dump() : function to save a Python object into a file using pickle serialization.\n",
        "\n",
        "\n",
        "- 'wb': which stands for write mode in binary format since we are dealing with non-textual data like serialized objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COuMCMBYcpHO"
      },
      "source": [
        "Here we load all your stored feature data to your project for quicker runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5Za0zmwcpHP"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/Flicker8k/features.pkl', 'rb') as read:\n",
        "    loaded_features = pickle.load(read)\n",
        "\n",
        "'''After executing this code snippet, you should have your features stored in loaded_features, ready for further processing or analysis.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qRMzXs6cpHR"
      },
      "source": [
        "## Load caption data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92NUxAxHcpHR"
      },
      "source": [
        "Here we store the captions data from the text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6gIxIsncpHS"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('/content/drive/My Drive/Flicker8k', 'captions.txt'), 'rb') as cap:\n",
        "    next(cap)\n",
        "    captions_doc = cap.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-34iF3_-cpHT"
      },
      "source": [
        "Splitting and appending the captions data with the image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU9ObiTwcpHT"
      },
      "outputs": [],
      "source": [
        "mapping = {}\n",
        "for line in tqdm(captions_doc.decode('UTF-8').split('\\n')):\n",
        "    tokens = line.split(',')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    # remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    # store the caption\n",
        "    mapping[image_id].append(caption.strip())\n",
        "\n",
        "'''After executing this code, you will have a dictionary named mapping, where each image ID is mapped to a list of its corresponding captions.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggwHxNZ6cpHU"
      },
      "source": [
        "- mapping{} : Empty dictionary that will be used to store the mapping between image IDs and captions\n",
        "\n",
        "- The loop iterates over each line in (captions_doc), which is document containing multiple lines of captions\n",
        "\n",
        "- Each line is split into separate tokens using (,) as the delimiter, resulting in a list called tokens\n",
        "\n",
        "- If the length of the current line (caption) is less than 2 characters, meaning it might be an empty or invalid entry, we skip processing it with continue\n",
        "\n",
        "- The first token (tokens[0]) represents the image ID, while all subsequent tokens (tokens[1:]) make up the caption itself\n",
        "\n",
        "- The file extension from the image ID (if present) is removed by splitting on '.' and taking only index 0\n",
        "\n",
        "- The list of words representing a single caption is joined together using \" \".join(caption) to create a single string for readability\n",
        "\n",
        "- If this particular image_id has not been encountered before (not present as key in mapping), an empty list is created as its value within mapping\n",
        "\n",
        "- The caption is appended to the list associated with the respective image_id in mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N0eMMVncpHV"
      },
      "source": [
        "Let's ensure of number of images loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BMl2KDxcpHW"
      },
      "outputs": [],
      "source": [
        "len(mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sZP17MocpHX"
      },
      "source": [
        "# Cleaning Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0PJHazKcpHX"
      },
      "source": [
        "Here we start cleaning captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4_GD-z6cpHY"
      },
      "outputs": [],
      "source": [
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc.,\n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # delete additional spaces\n",
        "            caption = caption.replace('\\s+', ' ')  # replaces consecutive whitespace characters with a single space.\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            print(caption)\n",
        "            captions[i] = caption\n",
        "clean(mapping)\n",
        "\n",
        "'''After executing this function on your input data, it will update and modify your original mapping by cleaning and preprocessing all captions stored within it.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4I_dBuhcpHZ"
      },
      "source": [
        "- \" \".join([word for word in caption.split() if len(word)>1]) : splits the cleaned-up-caption into words, discards words with length less than or equal to one, and rejoins them with spaces between them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jiSDaHicpHs"
      },
      "outputs": [],
      "source": [
        "mapping['1002674143_1b742ab4b8']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUPXhxilcpH0"
      },
      "outputs": [],
      "source": [
        "all_captions = []\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        all_captions.append(caption)\n",
        "len(all_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9NJfmaTcpH1"
      },
      "source": [
        "# Preprocessing of text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RJHqVhMcpH2"
      },
      "source": [
        "Tokenization process using the Tokenizer class from Keras in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egtcH5aicpH3"
      },
      "outputs": [],
      "source": [
        "# Tokenaization\n",
        "tokenizer = Tokenizer()   # creates an instance of the Tokenizer class\n",
        "tokenizer.fit_on_texts(all_captions)   # fits the tokenizer on a list or array\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "'''After executing these lines, you will have a tokenizer object (tokenizer) that has been fitted on your captions data and a variable (vocab_size)\n",
        "representing the total number of unique words found in your captions dataset plus one.\n",
        "Tokenization is an important preprocessing step used to convert text into numerical representations suitable for\n",
        "machine learning models, such as neural networks, where input data needs to be represented as numbers rather than raw text.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3EQPv3xGEka"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/Flicker8k/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVs6tvMkcpH4"
      },
      "source": [
        "- tokenizer.fit_on_texts(all_captions) : The purpose of this step is to build a vocabulary based on all unique words present in your corpus of captions\n",
        "\n",
        "- vocab_size = len(tokenizer.word_index) + 1 : This retrieves the size of the vocabulary by accessing the attribute word_index from the tokenizer, which contains a dictionary mapping words to their corresponding integer indices. Adding 1 accounts for an additional index that may be reserved for unknown or out-of-vocabulary (OOV) tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng5yyv9HcpH5"
      },
      "source": [
        "Finding the maximum length of the captions, used for reference for the padding sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCFBXdClcpH6"
      },
      "outputs": [],
      "source": [
        "# Getting maximum length\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvW6zc03cpH6"
      },
      "source": [
        "# Train Test Split\n",
        "Splitting a list of image IDs into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fGwAe2dcpH7"
      },
      "outputs": [],
      "source": [
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]\n",
        "\n",
        "'''After executing these lines, you will have two lists: 'train', containing approximately 90% of your original image IDs for\n",
        "training purposes, and 'test', containing approximately 10% for testing/evaluation purposes.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e323SuvcpH8"
      },
      "source": [
        "- image_ids = list(mapping.keys()) : This converts the keys of the dictionary mapping into a list called image_ids. The assumption here is that mapping is a dictionary where the keys represent some form of identifier for images\n",
        "\n",
        "- split = int(len(image_ids) * 0.90) : This calculates the index at which to split the data based on 90% (0.90) of the total number of elements in image_ids. The result, stored in variable split, represents approximately 90% of the length or size of image_ids\n",
        "\n",
        "- train = image_ids[:split] : This assigns to variable train a slice from index 0 up to (but not including) index specified by variable split. It contains approximately 90% of the elements from original list, representing your training set\n",
        "\n",
        "- test = image_ids[split:] : This assigns to variable 'test' a slice starting from index specified by variable 'split' until end (:). It contains remaining approximately 10% elements from original list, representing your test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6xCn5KAcpH8"
      },
      "source": [
        "Define batch including padding sequance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1CB1FOcpH9"
      },
      "outputs": [],
      "source": [
        "# create data generator to get data in batch (avoids session crash)\n",
        "def Data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # loop over images\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "                    # store the sequences\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0\n",
        "\n",
        "'''This data generator enables you to generate batches of image features, input sequences, and output labels on-the-fly while training your\n",
        "model in order to avoid memory issues or session crashes caused by loading all data into memory at once.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfP-WrKecpH9"
      },
      "source": [
        "- Data_generator --> function takes several input parameters : data_keys -> which represents the keys used to identify images; mapping -> a dictionary that maps each image key to its corresponding list of captions; features -> a dictionary that stores features extracted from the images; tokenizer -> an instance of tokenizer used to convert text into sequences of integers; max_length -> representing the maximum length allowed for input sequences; vocab_size -> indicating the total number of unique words in your vocabulary, and finally, batch_size -> determines how many samples are included in each batch\n",
        "\n",
        "- Inside the function, three empty lists (X1,X2,y) are initialized to store inputs (image features), partial sequence inputs (padded and encoded) and corresponding output labels respectively\n",
        "\n",
        "- A while loop with condition 1 is created to continuously generate batches\n",
        "\n",
        "- For each key value present in 'data_keys', this loop iterates through all captions associated with that key using another nested loop\n",
        "\n",
        "- Within this nested loop : Each caption is tokenized using .texts_to_sequences() method from tokenizer, The resulting sequence is split into pairs of input-output pairs by iterating over it, Input sequences up until position i-1 (in_seq) and output sequence at position i (out_seq) are obtained, The input sequence (in_seq) is padded using .pad_sequences() method so they all have equal length (max_length), Output sequence (out_seq) is one-hot encoded using .to_categorical() method\n",
        "\n",
        "- In every iteration, these processed sequences along with their respective image features are appended to their corresponding lists: X1 (image features), X2 (input sequences), and y (output sequences).\n",
        "\n",
        "- After batch_size iterations, the lists are converted to numpy arrays (X1, X2, y) using np.array().\n",
        "\n",
        "- The generated batch is yielded using the keyword yield. This allows the function to return a generator object that can be iterated upon during model training\n",
        "\n",
        "- Finally, once a batch has been yielded, the lists are reset back to empty and n is set back to 0 so that subsequent batches can be generated correctly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts65v_JzcpH-"
      },
      "source": [
        "# Model Creation (Defining the CNN & RNN Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMP7OK9lcpH-"
      },
      "source": [
        "Building a neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSZQsdVscpH_"
      },
      "outputs": [],
      "source": [
        "inputs1 = Input(shape=(2048,))    # Creating an input layer, each input has 2048 dimensions\n",
        "fe1 = Dropout(0.4)(inputs1) # 40% of the inputs will be randomly set to 0 during training to reduce overfitting.\n",
        "fe2 = Dense(256, activation='relu')(fe1) # This adds a fully connected dense layer with 256 units and ReLU activation function on top of the previous layer fe1.\n",
        "\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)  # Utilizes Long Short-Term Memory (LSTM), with 256 memory cells/neurons applied on se2.\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])  # Adds both fe2 and se3 layers element-wise together as part of decoder architecture.\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2) # The final output layer consists of a dense layer with vocab_size units\n",
        "                                                            # (representing the number of classes or vocabulary size) and softmax activation function to obtain\n",
        "                                                             # probability distribution over different classes/words.\n",
        "\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# visualize the model\n",
        "plot_model(model, show_shapes=True)\n",
        "\n",
        "'''this code snippet provides an overview of building a neural network using Keras but does not include details about data preprocessing or\n",
        "            training steps which may be required before fitting this model to actual data.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbj4Vh56cpIA"
      },
      "source": [
        "- shape=(2048,) : Denoting that it is a tuple with only one element, (2048) -> Size of dimensionality\n",
        "\n",
        "- se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2) : It performs embedding on the second set of inputs using an embedding layer with vocab_size vocabulary size and 256-dimensional embeddings per word/token.The mask_zero parameter indicates whether or not to skip padding values (zeros).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxZLmJR-cpIA"
      },
      "source": [
        "# Training The Model\n",
        "\n",
        "Now let's train our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6WFHlnjcpIB"
      },
      "outputs": [],
      "source": [
        "\n",
        "epochs = 25\n",
        "batch_size = 32\n",
        "# Calculate number of steps per epoch\n",
        "steps = len(train) // batch_size   #  Back propagation and fetch the next data\n",
        "\n",
        "for i in range(epochs):\n",
        "    generator = Data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1) # verbose=1 -> controls whether progress updates are displayed during training.\n",
        "\n",
        "'''This setup allows you to train your model on large datasets efficiently by processing smaller batches at once instead of loading all data into memory simultaneously.\n",
        "                    - Loss function decreases gradually over the iterations,\n",
        "                    - Increase the no. of epochs for better results,\n",
        "                    - Assign the no. of epochs and batch size accordingly for quicker results'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyaHbgeWcpIC"
      },
      "source": [
        "- Epochs: An epoch represents one complete pass through the entire training dataset during training. Increasing the number of epochs allows the model to see more instances and potentially improve its performance. However, too many epochs can lead to overfitting if not regularized properly. It is common to start with a smaller number of epochs and gradually increase it until desired performance is achieved or early stopping criteria are met.You could try using 10-100 epochs depending on convergence behavior observed during training/validation phases.\n",
        "\n",
        "- Batch Size: Batch size refers to how many samples are processed together before updating model weights during each iteration (or mini-batch) in an epoch.Smaller batch sizes allow for more frequent weight updates but may slow down training due to increased computational overhead. Larger batch sizes speed up training but may result in less accurate updates since they represent an average over multiple examples.Typical values range from 32 up to 256 or even higher based on available memory capacity.\n",
        "\n",
        "- Finally There is no universally perfect value for these parameters as it heavily depends on your specific task and data characteristics. You will need to experiment with different values and monitor their impact on metrics like loss/error rate or validation accuracy to find what works best for your particular case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPKlCGG-cpID"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRiM9zEXcpID"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/My Drive/Flicker8k'+'/model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqMUK-vFcpIE"
      },
      "source": [
        "# Generate Captions\n",
        "\n",
        "Let's generate captions for images using (index_to_word) function that aims to convert an index back to its corresponding word based on the word-to-index mapping stored in the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dmwhEYJcpIF"
      },
      "outputs": [],
      "source": [
        "def index_to_word(integer, tokenizer): # integer -> the index to be converted\n",
        "                                       # tokenizer -> the object containing the word-to-index mapping\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "'''This function can be useful when working with natural language processing tasks or\n",
        "text data where we want to map numerical indices back to their original words using a pre-built dictionary or tokenizer.\n",
        "Convert the predicted index from the model into a word'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgbGyZnBcpIF"
      },
      "outputs": [],
      "source": [
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):  # this determines the maximum length of generated captions).\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0) # represents probabilities across different words in vocabulary.\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat) # The index with highest probability (argmax) is selected as the predicted word index.\n",
        "        # convert index to word\n",
        "        word = index_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    in_text = in_text.replace('startseq', '').replace('endseq', '')\n",
        "    return in_text.strip()\n",
        "\n",
        "'''Generates captions for images using a pre-trained model, tokenizer, and maximum sequence length using (predict_caption) function'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Py9wrfBcpIH"
      },
      "source": [
        "# Validation\n",
        "\n",
        "Validating the data using BLEU Score\n",
        "\n",
        "The BLEU (Bilingual Evaluation Understudy) score is a metric commonly used to evaluate the quality of machine-generated translations or summaries by comparing them against one or more reference translations. It measures the similarity between an automatically generated translation and human-generated reference translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99_Z50CAcpII"
      },
      "outputs": [],
      "source": [
        "# validate with test data\n",
        "actual, predicted = list(), list() # Score actual captions and predicted captions for each image in the test dataset\n",
        "# This loop iterates over each image in the test dataset (represented by test) using the tqdm function to display a progress bar.\n",
        "for key in tqdm(test):\n",
        "    # get actual caption\n",
        "    captions = mapping[key]\n",
        "    # predict the caption for image\n",
        "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
        "    # actual and predicted captions are split into individual words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    y_pred = y_pred.split()\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)\n",
        "# calcuate BLEU score\n",
        "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))) # unigram precision\n",
        "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))) # bigram precision\n",
        "\n",
        "'''this code evaluates how well a caption generation model performs on test data by comparing its\n",
        "generated captions to reference or ground truth captions using BLEU scores.'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlmMqscicpIJ"
      },
      "source": [
        "- A BLEU Score more than 0.4 is considered a good result, for a better score increase the number of epochs accordingly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G_A7cI5cpIK"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7CacVGscpIK"
      },
      "outputs": [],
      "source": [
        "def generate_caption(image_name):\n",
        "    # load the image\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join('/content/drive/My Drive/Flicker8k', \"Images\", image_name)\n",
        "    image = Image.open(img_path)\n",
        "    captions = mapping[image_id]\n",
        "    print('Actual caption : ')\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
        "    print('Predicted caption : ')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohxOw6L-cpIL"
      },
      "outputs": [],
      "source": [
        "generate_caption('99171998_7cc800ceef.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi1ICp1TcpIM"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"72218201_e0e9c7d65b.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP3OFqUKcpIN"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"55473406_1d2271c1f2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMe_wfDBcpIP"
      },
      "outputs": [],
      "source": [
        "generate_caption(\"99679241_adc853a5c0.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWf-x7vIcpIR"
      },
      "source": [
        "# Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e544N2RxcpIR"
      },
      "source": [
        "This project aimed to develop a deep learning model capable of generating descriptive captions for images using the Flicker8k dataset. By leveraging convolutional neural networks (CNNs) for image feature extraction and recurrent neural networks (RNNs) with attention mechanisms for language generation, we have successfully built an image caption generator.Throughout the project, we preprocessed the Flicker8k dataset by extracting features from images using a pre-trained CNN model. These features were then used as inputs to an RNN-based architecture that learned to generate captions based on those visual representations.The model was trained on a large number of image-caption pairs and optimized using techniques like teacher forcing and beam search decoding. We carefully fine-tuned hyperparameters, experimented with different architectures, and conducted thorough evaluations to ensure accurate and meaningful caption generation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}